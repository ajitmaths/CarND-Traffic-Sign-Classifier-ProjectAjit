{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Traffic Sign Recognition**\n",
    "\n",
    "### **Goal Build a Traffic Sign Recognition Project**\n",
    "\n",
    "Steps of this project are the following:\n",
    "* Load the data set (see below for links to the project data set)\n",
    "* Explore, summarize and visualize the data set\n",
    "* Design, train and test a model architecture\n",
    "* Use the model to make predictions on new images\n",
    "* Analyze the softmax probabilities of the new images\n",
    "\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./model_output/Validationaccuracy.png \"Validation Accuracy\"\n",
    "[image2]: ./model_output/Trainaccuracy.png \"Train Accuracy\"\n",
    "[image3]: ./examples/random_noise.jpg \"Random Noise\"\n",
    "[image4]: ./test_Images/1.png \"Traffic Sign 1\"\n",
    "[image5]: ./test_Images/2.png \"Traffic Sign 2\"\n",
    "[image6]: ./test_Images/3.png \"Traffic Sign 3\"\n",
    "[image7]: ./test_Images/4.png \"Traffic Sign 4\"\n",
    "[image8]: ./test_Images/5.png \"Traffic Sign 5\"\n",
    "[image9]: ./test_Images/6.png \"Traffic Sign 6\"\n",
    "[image10]: ./test_Images/7.png \"Traffic Sign 7\"\n",
    "[image11]: ./test_Images/8.png \"Traffic Sign 8\"\n",
    "[image12]: ./model_output/RawandProcessedOutput1.png \"Raw and Processed Output\"\n",
    "[image13]: ./model_output/RawandProcessedOutput2.png \"Raw and Processed Output\"\n",
    "[image14]: ./model_output/RawandProcessedOutput3.png \"Raw and Processed Output\"\n",
    "[image15]: ./model_output/TrainandValidationGreyscale.png \"Train and Validation Greyscale\"\n",
    "[image16]: ./model_output/ValidDatasetSignCounts.png \"Validation Data Set Counts\"\n",
    "[image17]: ./model_output/TrainDatasetSignCounts.png \"Train Dataset Counts\"\n",
    "[image18]: ./model_output/TestDatasetSignCounts.png \"Test Dataset Counts\"\n",
    "[image19]: ./model_output/ValidDatasetSignCounts1.png \"Validation Data Set Counts\"\n",
    "[image20]: ./model_output/TrainDatasetSignCounts1.png \"Train Dataset Counts\"\n",
    "[image21]: ./model_output/TestDatasetSignCounts1.png \"Test Dataset Counts\"\n",
    "[image22]: ./model_output/mean_variance.png \"Normalization\"\n",
    "[image23]: ./model_output/MyTestImages1.png \"Test Images Results 1\"\n",
    "[image24]: ./model_output/MyTestImages2.png \"Test Images Results 2\"\n",
    "[image25]: ./model_output/TestImagesbeforeandafter.png \"TestImagesbeforeandafter\"\n",
    "[image26]: ./model_output/MyTestImages5.png \"Test Images Results 1\"\n",
    "[image27]: ./model_output/MyTestImages6.png \"Test Images Results 2\"\n",
    "[image28]: ./model_output/MyTestimagesbar1.png \" Test Images Bar 1\"\n",
    "[image29]: ./model_output/MyTestimagesbar2.png \" Test Images Bar 2\"\n",
    "[image30]: ./model_output/outputFeatureMapLayer1.png \" Layer 1 Feature Map\"\n",
    "[image31]: ./model_output/outputFeatureMapLayer2.png \" Layer 2 Feature Map\"\n",
    "[image32]: ./model_output/outputFeatureMapLayer3_1.png \" Layer 1 Feature Map Part1\"\n",
    "[image33]: ./model_output/outputFeatureMapLayer3_2.png \" Layer 1 Feature Map Part1\"\n",
    "[image34]: ./model_output/outputFeatureMapLayer3_3.png \" Layer 1 Feature Map Part1\"\n",
    "[image35]: ./model_output/outputFeatureMapLayer3_4.png \" Layer 1 Feature Map Part1\"\n",
    "[image36]: ./model_output/outputFeatureMapLayer3_5.png \" Layer 1 Feature Map Part1\"\n",
    "[image37]: ./model_output/outputFeatureMapLayer3_6.png \" Layer 1 Feature Map Part1\"\n",
    "[image38]: ./model_output/outputFeatureMapLayer3_7.png \" Layer 1 Feature Map Part1\"\n",
    "[image39]: ./model_output/MyTestimagesbar3.png \" Test Images Bar 3\"\n",
    "[image40]: ./model_output/MyTestimagesbar4.png \" Test Images Bar 4\"\n",
    "[image41]: ./model_output/MyTestImages3.png \"Test Images Results 1\"\n",
    "[image42]: ./model_output/MyTestImages4.png \"Test Images Results 2\"\n",
    "\n",
    "\n",
    "### External Links References\n",
    "[Traffic Sign Recognition with Multi-Scale Convolutional Networks](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf)  \n",
    "[End-to-End Deep Learning for Self-Driving Cars](https://devblogs.nvidia.com/deep-learning-self-driving-cars/)  \n",
    "\n",
    "## README\n",
    "\n",
    "\n",
    "### Data Set Summary & Exploration\n",
    "\n",
    "#### 1. Provide a basic summary of the data set. In the code, the analysis should be done using python, numpy and/or pandas methods rather than hardcoding results manually.\n",
    "\n",
    "I used the pandas library to calculate summary statistics of the traffic signs data set. The pickled data is a dictionary with 4 key/value pairs:\n",
    "'features' is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "'labels' is a 1D array containing the label/class id of the traffic sign. The file signnames.csv contains id -> name mappings for each id.\n",
    "'sizes' is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "'coords' is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. Pickled data is 32*32 images.\n",
    "\n",
    "Training subset is 80% of initial train.p dataset randomly obtained, validation subset is the rest of the raw train dataset. For testing the full test.p dataset is used.\n",
    "\n",
    "Training Set:   31367 samples (31367, 32, 32, 3) shape\n",
    "Validation Set: 7842 samples (7842, 32, 32, 3) shape\n",
    "Test Set:       12630 samples (12630, 32, 32, 3) shape\n",
    "Sign Classes Label: 43 samples\n",
    "\n",
    "#### 2. Include an exploratory visualization of the dataset.\n",
    "\n",
    "Here is an exploratory visualization of the data set. It is a bar chart showing how the data is distributed across the Train, Validation and Test sets per class.\n",
    "\n",
    "| Train Dataset         |     Validation Dataset\t|    Test Dataset\n",
    "|:---------------------:|:-------------------------:|:-------------------------:|\n",
    "|![alt text][image19]   |![alt text][image20]\t\t|![alt text][image21]  \t    |\n",
    "\n",
    "\n",
    "Total number of images in the augmented dataset =  31367  \n",
    "Total number of images in the augmented dataset =  7842  \n",
    "Total number of images in the train dataset =  62734  \n",
    "Total number of images in the valid dataset =  15684  \n",
    "Features Normalized  \n",
    "Labels One-Hot Encoded  \n",
    "Training Set:   62734 samples (62734, 32, 32, 1) shape  \n",
    "Validation Set: 15684 samples (15684, 32, 32, 1) shape  \n",
    "Test Set:       12630 samples (12630, 32, 32, 1) shape  \n",
    "\n",
    "Image Shape: (32, 32, 1)  \n",
    "Sign Classes Label: 43 samples  \n",
    "\n",
    "\n",
    "### Design and Test a Model Architecture\n",
    "\n",
    "#### 1. Describe how you preprocessed the image data. What techniques were chosen and why did you choose these techniques? Consider including images showing the output of each preprocessing technique. Pre-processing refers to techniques such as converting to grayscale, normalization, etc. (OPTIONAL: As described in the \"Stand Out Suggestions\" part of the rubric, if you generated additional data for training, describe why you decided to generate additional data, how you generated the data, and provide example images of the additional data. Then describe the characteristics of the augmented training set like number of images in the set, number of images for each class, etc.)\n",
    "Following are the image preprocessing steps:\n",
    "a) Image Transformation. This step sharpens and increases the contrast of the Images. \n",
    "\n",
    "| Train Dataset         |     Validation Dataset\t|    Test Dataset\n",
    "|:---------------------:|:-------------------------:|:-------------------------:|\n",
    "|![alt text][image12]   |![alt text][image13]\t\t|![alt text][image14]  \t    |\n",
    "\n",
    "Additionally, I build a jittered dataset by adding per image transformed versions of the original training set, yielding __62734__ samples in total. Samples are randomly perturbed.\n",
    "Below is the split of datasets after augmentation. By adding them synthetically will yield more robust learning to potential deformations in the test set. This also increased the number of train data for the classes which had lesser training data.\n",
    "\n",
    "| Train Dataset         |     Validation Dataset\t|    Test Dataset\n",
    "|:---------------------:|:-------------------------:|:-------------------------:|\n",
    "|![alt text][image19]   |![alt text][image20]\t\t|![alt text][image21]  \t    |\n",
    "\n",
    "I tested the model with and without these synthetic images -\n",
    "\n",
    "Model with out these images  yielded higher validation accuracy. But the Test Data Accuracy is around 93.8%.\n",
    "\n",
    "EPOCH 30 ...  \n",
    "Train Accuracy = 1.000  \n",
    "Validation Accuracy = 0.994  \n",
    "\n",
    "Model saved  \n",
    "INFO:tensorflow:Restoring parameters from ./lenet  \n",
    "Test Data Set Accuracy = 0.938  \n",
    "\n",
    "Model with these synthetic images  yielded higher test data accuracy. But the Train and Validation accuracy was 98.4% and 96.8% \n",
    "\n",
    "EPOCH 30 ...\n",
    "Train Accuracy = 0.984\n",
    "Validation Accuracy = 0.968\n",
    "\n",
    "Model saved\n",
    "INFO:tensorflow:Restoring parameters from ./lenet\n",
    "Test Data Set Accuracy = 0.940\n",
    "\n",
    "\n",
    "b) I decided to convert the images to grayscale because intensity (e.g. edge detection) plays a major role. Grayscale is usually sufficient to distinguish such edges. \n",
    "\n",
    "Here is an example of a traffic sign images after grayscaling.\n",
    "![alt text][image15]\n",
    "\n",
    "c) As a last step, I normalized the image data because to get to zero mean and equal variance.\n",
    "![alt text][image22]\n",
    "\n",
    "Normalization is done by Min-Max scaling to a range of a=0.1 and b=0.9. After scaling, the values of the pixels in the input data should range from 0.1 to 0.9. Since the image data is in grayscale, the current values range from a min of 0 to a max of 255.\n",
    "\n",
    "__Min-Max Scaling:  X′=a+(X−Xmin)/(b−a)Xmax−XminX′=a+(X−Xmin)/(b−a)Xmax−Xmin\n",
    "\n",
    "#### 2. Describe what your final model architecture looks like including model type, layers, layer sizes, connectivity, etc.) Consider including a diagram and/or table describing the final model.\n",
    "\n",
    "The CNN architecture was inspired by __LeNet__. My final model consisted of the following layers:\n",
    "\n",
    "| Layer         \t\t|     Description\t        \t\t\t\t\t                |\n",
    "|:---------------------:|:-------------------------------------------------------------:|\n",
    "| Input         \t\t| 32x32x1 Greyscale image   \t\t\t\t\t                |\n",
    "| Convolution 5x5     \t| shape=(5, 5, 1, 6) 1x1 stride, same padding, Outputs 28x28x6 \t|\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t                |\n",
    "| Max pooling\t      \t| 2x2 stride,  Outputs 14x14x6   \t\t\t\t                |\n",
    "| Convolution 5x5\t    | shape=(5, 5, 6, 16) 1x1 stride, same padding, Outputs 10x10x16|\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t                |\n",
    "| Max pooling\t      \t| 2x2 stride,  Outputs 5x5x6   \t\t\t     \t                |\n",
    "| Convolution 4x4\t    | shape=(4, 4, 16, 412) Outputs 2x2x412\t\t    \t            |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t                |\n",
    "| Max pooling\t      \t| 2x2 stride,  Outputs 1x1x412   \t\t\t\t                |\n",
    "| Fully connected\t\t| Output 122  \t\t            \t\t\t\t                |\n",
    "| Fully connected\t\t| Output 84  \t\t            \t\t\t\t                |\n",
    "| Fully connected\t\t| Output 43  \t\t        \t\t\t\t                    |\n",
    "| Softmax\t\t\t\t| 43 Classes    \t\t\t\t\t\t\t\t\t            | \n",
    "\n",
    "\n",
    "#### 3. Describe how you trained your model. The discussion can include the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate.\n",
    "\n",
    "To train the model, I used __AdamOptimizer__ basically as a motivation from the LeNet.\n",
    "\n",
    "The ``tf.train.AdamOptimizer`` uses Adam algorithm to control the learning rate. Adam offers several advantages over the simple ```tf.train.GradientDescentOptimizer```.  Stochastic gradient descent which maintains a single learning rate for all weight updates and the learning rate does not change during training. Foremost is that it uses moving averages of the parameters (momentum). Simply put, this enables Adam to use a larger effective step size, and the algorithm will converge to this step size without fine tuning. The main down side of the algorithm is that Adam requires more computation to be performed for each parameter in each training step (to maintain the moving averages and variance, and calculate the scaled gradient); and more state to be retained for each parameter (approximately tripling the size of the model to store the average and variance for each parameter).   \n",
    "__BATCH_SIZE__ = 156 Batch size is the number of training examples in one forward/backward pass.    \n",
    "\n",
    "__EPOCHS__ = 30 Epoch is one forward pass and one backward pass of __all__ the training examples.     \n",
    "\n",
    "__Learning Rate__ = .00097. The learning rate used is .00097. The proportion that weights are updated (e.g. 0.001). Larger values (e.g. 0.3) results in faster initial learning before the rate is updated. Smaller values (e.g. 1.0E-5) slow learning right down during training.  \n",
    "\n",
    "Variables were initialized with using of a truncated normal distribution with mu = 0.0 and sigma = 0.1. Learning rate was finetuned by try and error process.  \n",
    "\n",
    "Traffic sign classes were coded into one-hot encodings.  \n",
    "\n",
    "As one can observe, at the end of the training process, accuracy stopped increasing and loss oscillated around relatively small value.  \n",
    "\n",
    "\n",
    "\n",
    "#### 4. Describe the approach taken for finding a solution and getting the validation set accuracy to be at least 0.93. Include in the discussion the results on the training, validation and test sets and where in the code these were calculated. Your approach may have been an iterative process, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think the architecture is suitable for the current problem.\n",
    "\n",
    "My final model results were:\n",
    "* training set accuracy of ? __98.3%__\n",
    "* validation set accuracy of ? __96.7%__\n",
    "* test set accuracy of ? __94.2%__\n",
    "\n",
    "| Training Set Accuracy |     Validation Set Acuracy|   \n",
    "|:---------------------:|:-------------------------:|\n",
    "|![alt text][image1]   |![alt text][image2]\t\t    |\n",
    "\n",
    "If an iterative approach was chosen:\n",
    "* What was the first architecture that was tried and why was it chosen?   \n",
    "First architecture choosen only had two convolution layer (based of LeNet). I added the third convolution layer with the idea  go through more conv layers, so as to  get activation maps that represent more and more complex features. This resulted in a Test Images which had higher success rates.\n",
    "* What were some problems with the initial architecture?   \n",
    "Problem with the initial architecture was underfitting. By adding additional layer i was able to get to higher than 95% training accuracy.\n",
    "I also added Dropouts with keepprob = 0.5 for Training and 1.0 for Validation. This helped overfitting during training.\n",
    "\n",
    "* How was the architecture adjusted and why was it adjusted? Typical adjustments could include choosing a different model architecture, adding or taking away layers (pooling, dropout, convolution, etc), using an activation function or changing the activation function. One common justification for adjusting an architecture would be due to overfitting or underfitting. A high accuracy on the training set but low accuracy on the validation set indicates over fitting; a low accuracy on both sets indicates under fitting.    \n",
    "As described above added additional convolution layer and dropouts. One additional improvement could be addition of inception modules. Inception Module would increase the performance on such kind of tasks as they allow to not select optimal layer (say, convolution 5x5 or 3x3), by performing different layer types simultaneously and selecting the best one on its own.\n",
    "\n",
    "* Which parameters were tuned? How were they adjusted and why?   \n",
    "Parameters tuned included Learning Rate and Epochs. Learning Rate tuning parameters and Number of layers in the Model.\n",
    "* What are some of the important design choices and why were they chosen? For example, why might a convolution layer work well with this problem? How might a dropout layer help with creating a successful model?  \n",
    "\n",
    "* What architecture was chosen?     \n",
    "LeNet with some additional modifications was chosen.  \n",
    "* Why did you believe it would be relevant to the traffic sign application?  \n",
    "Given that LeNet is able to classify the greyscale handwritten images, it was a good starting point for the Traffic sign classifier.   \n",
    "* How does the final model's accuracy on the training, validation and test set provide evidence that the model is working well?    \n",
    "When i started the model the speed of training and validation accuracy was very slow. However addition of new convolution layer improved the training and validation accuracy. I also noticed that after adding synthetic images to the data set the model became more robuts to preduict the test images.\n",
    "\n",
    "### Test a Model on New Images\n",
    "\n",
    "#### 1. Choose five German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify.\n",
    "\n",
    "Here are eight German traffic signs that i used to test the model\n",
    "\n",
    "| __German Traffic Signs__  |     \t        \t\t\t\t\t            |\n",
    "|:-------------------------:|:---------------------------------------------:|\n",
    "|![alt text][image4]     \t|![alt text][image8]  \t\t\t\t\t\t\t|\n",
    "|![alt text][image5]        |![alt text][image9]                            |\n",
    "|![alt text][image6]  \t    |![alt text][image10] \t\t\t\t\t\t\t|\n",
    "|![alt text][image7]\t    |![alt text][image11]\t\t\t\t\t\t\t|   \n",
    "\n",
    "\n",
    "Below are the test results of the test images classifications.\n",
    "  \n",
    "|   __First Trial__     |\n",
    "|:---------------------:|\n",
    "![alt text][image23]\n",
    "![alt text][image24]\n",
    "\n",
    "Its is observed that this result could vary run by run. For example, in the below run i observed that the model having difficulty with the \"Bumpy Road\" sign. This could be resolved having more training data with Image augemented. \n",
    "One of the test images \"Bumpy Road\" was predicted to be a \"Road Work\" sign. \"Bump Road\" had Low Recall i.e the model has trouble predicting on stop signs.  \n",
    "\n",
    "|   __Second Trial__    |\n",
    "|:---------------------:|\n",
    "![alt text][image41]\n",
    "![alt text][image42]\n",
    "\n",
    "  \n",
    "\n",
    "|   __Third Trial__    |\n",
    "|:---------------------:|\n",
    "![alt text][image26]\n",
    "![alt text][image27]\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Discuss the model's predictions on these new traffic signs and compare the results to predicting on the test set. At a minimum, discuss what the predictions were, the accuracy on these new predictions, and compare the accuracy to the accuracy on the test set (OPTIONAL: Discuss the results in more detail as described in the \"Stand Out Suggestions\" part of the rubric).\n",
    "\n",
    "Here are the results of the prediction:\n",
    "\n",
    "|   __First Trial__                                                     |\n",
    "|:---------------------:|:---------------------------------------------:|\n",
    "| __Image__\t\t\t    |     __Prediction__\t                 \t\t|\n",
    "|                                                                       |\n",
    "| 30 Km/h      \t\t    | 30 Km/h   \t\t\t\t\t\t\t\t\t|\n",
    "| Bumpy Road    \t\t| Bumpy Road \t\t\t\t\t\t\t\t\t|\n",
    "| Ahead Only\t\t\t| Ahead Only\t\t\t\t\t\t\t\t\t|\n",
    "| No Vehicles\t     \t| No Vehicles\t\t\t\t\t \t\t\t\t|\n",
    "| Go strainght or left\t| Go strainght or left    \t\t\t\t\t\t|\n",
    "| General caution\t    | General caution\t\t\t\t\t\t\t\t|\n",
    "| 30 Km/h\t      \t\t| 30 Km/h\t\t\t\t\t \t\t\t\t    |\n",
    "| Keep Left\t\t\t    | Keep Left      \t\t\t\t\t\t\t    |\n",
    "\n",
    "The model was able to correctly guess 8 of the 8 traffic signs, which gives an accuracy of 100%. This compares favorably to the accuracy on the test set of 94%.  \n",
    "\n",
    "|   __Second Trial__                                                    |\n",
    "|:---------------------:|:---------------------------------------------:|\n",
    "| __Image__\t\t\t    |     __Prediction__\t        \t\t\t\t|\n",
    "|                                                                       |\n",
    "| 30 Km/h      \t\t    | 30 Km/h   \t\t\t\t\t\t\t\t\t|\n",
    "| Bumpy Road    \t\t| ~~Bumpy Road~~  \t\t\t\t\t\t\t    |\n",
    "| Ahead Only\t\t\t| Ahead Only\t\t\t\t\t\t\t\t\t|\n",
    "| No Vehicles\t     \t| No Vehicles\t\t\t\t\t \t\t\t\t|\n",
    "| Go strainght or left\t| Go strainght or left    \t\t\t\t\t\t|\n",
    "| General caution\t    | General caution\t\t\t\t\t\t\t\t|\n",
    "| 30 Km/h\t      \t\t| 30 Km/h\t\t\t\t\t \t\t\t\t    |\n",
    "| Keep Left\t\t\t    | Keep Left      \t\t\t\t\t\t\t    |\n",
    "\n",
    "The model was able to correctly guess 8 of the 8 traffic signs, which gives an accuracy of 84%.   \n",
    "\n",
    "|   __Third Trial__                                                     |\n",
    "|:---------------------:|:---------------------------------------------:|\n",
    "| __Image__\t\t\t    |     __Prediction__\t        \t\t\t\t|\n",
    "|                                                                       |\n",
    "| 30 Km/h      \t\t    | 30 Km/h   \t\t\t\t\t\t\t\t\t|\n",
    "| Bumpy Road    \t\t| Bumpy Road \t\t\t\t\t\t\t\t    |\n",
    "| Ahead Only\t\t\t| Ahead Only\t\t\t\t\t\t\t\t\t|\n",
    "| No Vehicles\t     \t| No Vehicles\t\t\t\t\t \t\t\t\t|\n",
    "| Go strainght or left\t| Go strainght or left    \t\t\t\t\t\t|\n",
    "| General caution\t    | General caution\t\t\t\t\t\t\t\t|\n",
    "| 30 Km/h\t      \t\t| 30 Km/h\t\t\t\t\t \t\t\t\t    |\n",
    "| Keep Left\t\t\t    | Keep Left      \t\t\t\t\t\t\t    |\n",
    "\n",
    "\n",
    "The model was able to correctly guess 8 of the 8 traffic signs, which gives an accuracy of __100__%. This compares favorably to the accuracy on the test set of 94%.\n",
    "\n",
    "#### 3. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction. Provide the top 5 softmax probabilities for each image along with the sign type of each probability. (OPTIONAL: as described in the \"Stand Out Suggestions\" part of the rubric, visualizations can also be provided such as bar charts)\n",
    "\n",
    "Below is the result of the model  when predicting on each of the  new images by looking at the softmax probabilities for each prediction. Also visualizations bar chart below.\n",
    "![alt text][image39]\n",
    "![alt text][image40]\n",
    "\n",
    "\n",
    "|   __First Trial__                                                 |\n",
    "|:---------------------:|:-----------------------------------------:|\n",
    "| __Prediction__\t    |     __Probability__    \t   \t\t\t\t|\n",
    "|                                                                   |\n",
    "| 30 Km/h      \t\t    | 98%  \t\t\t\t\t\t\t\t\t|\n",
    "| Bumpy Road    \t\t| 100%\t\t\t\t\t\t\t\t        |\n",
    "| Ahead Only\t\t\t| 100%\t\t\t\t\t\t\t\t\t    |\n",
    "| No Vehicles\t     \t| 100%\t\t\t\t\t \t\t\t\t    |\n",
    "| Go straight or left\t| 100%    \t\t\t\t\t\t            |\n",
    "| General caution\t    | 100%\t\t\t\t\t\t\t\t        |\n",
    "| 30 Km/h\t      \t\t| 100%\t\t\t\t\t \t\t\t\t    |\n",
    "| Keep Left\t\t\t    | 100%      \t\t\t\t\t\t\t    |\n",
    "\n",
    "\n",
    "![alt text][image28]\n",
    "![alt text][image29]\n",
    "\n",
    "|   __Second Trial__                                                 |\n",
    "|:---------------------:|:-----------------------------------------:|\n",
    "| __Prediction__\t    |     __Probability__    \t   \t\t\t\t|\n",
    "|                                                                   |\n",
    "| 30 Km/h      \t\t    | 100%  \t\t\t\t\t\t\t\t\t|\n",
    "| Bumpy Road    \t\t| 99%\t\t\t\t\t\t\t\t        |\n",
    "| Ahead Only\t\t\t| 100%\t\t\t\t\t\t\t\t\t    |\n",
    "| No Vehicles\t     \t| 89%\t\t\t\t\t \t\t\t\t    |\n",
    "| Go straingt or left\t| 8%    \t\t\t\t\t\t            |\n",
    "| General caution\t    | 100%\t\t\t\t\t\t\t\t        |\n",
    "| 30 Km/h\t      \t\t| 100%\t\t\t\t\t \t\t\t\t    |\n",
    "| Keep Left\t\t\t    | 100%      \t\t\t\t\t\t\t    |\n",
    "\n",
    "### (Optional) Visualizing the Neural Network (See Step 4 of the Ipython notebook for more details)\n",
    "#### 1. Discuss the visual output of your trained network's feature maps. What characteristics did the neural network use to make classifications?\n",
    "\n",
    "Below is the visualization of the Neural Network.\n",
    " \n",
    "Layer1 is able to look at the diagonal lines and staringht lines. It is able to show that their network's inner weights had high activations to the boundary lines. As visualized here, the first layer of the CNN can recognize -45 degree lines. The first layer of the CNN is also able to recognize +45 degree lines, like the one above. So, the first layer of our CNN clearly picks out very simple shapes and patterns like lines and blobs\n",
    "\n",
    "|   __Layer1__                                                      |\n",
    "|:---------------------:|:-----------------------------------------:|\n",
    "![alt text][image30]\n",
    "\n",
    "Layer2 is able activate different features such as Arrows. A visualization of the second layer we can see how it ispicking up more complex ideas like circles and arrows.\n",
    "\n",
    "|   __Layer2__                                                      |\n",
    "|:---------------------:|:-----------------------------------------:|\n",
    "![alt text][image31]\n",
    "\n",
    "Layer 3 tpicks out complex combinations of features from the second layer. These include things like grids.\n",
    "\n",
    "|   __Layer3__                                                      |\n",
    "|:---------------------:|:-----------------------------------------:|\n",
    "![alt text][image32]\n",
    "![alt text][image33]\n",
    "![alt text][image34]\n",
    "![alt text][image35]\n",
    "![alt text][image36]\n",
    "![alt text][image37]\n",
    "![alt text][image38]\n",
    "\n",
    "As we continue through the last layers it will pick the highest order ideas that we care about for classification, like different traffic classes."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
